{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    \n",
    "    def __init__(self, inp_dim, h1, h2, out_dim):\n",
    "        \n",
    "        super(NN,self).__init__()\n",
    "        self.linear1 = nn.Linear(inp_dim, h1)\n",
    "        self.linear2 = nn.Linear(h1, h2)\n",
    "        self.linear3 = nn.Linear(h2,out_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        out = F.softmax(self.linear3(x))\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN(4,8,6,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of NN(\n",
       "  (linear1): Linear(in_features=4, out_features=8, bias=True)\n",
       "  (linear2): Linear(in_features=8, out_features=6, bias=True)\n",
       "  (linear3): Linear(in_features=6, out_features=3, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../Python/Datasets/Iris.csv\", index_col='Id').sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,val in enumerate(data['Species'].unique()):\n",
    "    data.replace(val,i,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm  Species\n",
       "0            7.7           3.8            6.7           2.2        0\n",
       "1            5.8           2.7            5.1           1.9        0\n",
       "2            6.1           3.0            4.6           1.4        1\n",
       "3            6.7           3.1            4.7           1.5        1\n",
       "4            6.0           2.9            4.5           1.5        1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(0.7*data.shape[0])\n",
    "input_train, output_train = torch.from_numpy(data.values[:split,:-1]).float(), torch.from_numpy(data.values[:split,-1:]).long().view(-1)\n",
    "input_test, output_test = torch.from_numpy(data.values[split:,:-1]).float(), torch.from_numpy(data.values[split:,-1:]).long().view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([105, 4]), torch.Size([105]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_train.size(), output_train.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([45, 4]), torch.Size([45]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_test.size(), output_test.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadron/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 Loss 1.111 Acc 0.333\n",
      "Epoch : 1 Loss 1.091 Acc 0.333\n",
      "Epoch : 2 Loss 1.049 Acc 0.333\n",
      "Epoch : 3 Loss 0.978 Acc 0.390\n",
      "Epoch : 4 Loss 0.918 Acc 0.667\n",
      "Epoch : 5 Loss 0.864 Acc 0.667\n",
      "Epoch : 6 Loss 0.822 Acc 0.667\n",
      "Epoch : 7 Loss 0.804 Acc 0.667\n",
      "Epoch : 8 Loss 0.781 Acc 0.667\n",
      "Epoch : 9 Loss 0.755 Acc 0.867\n",
      "Epoch : 10 Loss 0.722 Acc 0.829\n",
      "Epoch : 11 Loss 0.676 Acc 0.981\n",
      "Epoch : 12 Loss 0.658 Acc 0.962\n",
      "Epoch : 13 Loss 0.657 Acc 0.962\n",
      "Epoch : 14 Loss 0.620 Acc 0.971\n",
      "Epoch : 15 Loss 0.615 Acc 0.962\n",
      "Epoch : 16 Loss 0.616 Acc 0.962\n",
      "Epoch : 17 Loss 0.593 Acc 0.971\n",
      "Epoch : 18 Loss 0.604 Acc 0.952\n",
      "Epoch : 19 Loss 0.589 Acc 0.971\n",
      "Epoch : 20 Loss 0.593 Acc 0.971\n",
      "Epoch : 21 Loss 0.584 Acc 0.981\n",
      "Epoch : 22 Loss 0.589 Acc 0.962\n",
      "Epoch : 23 Loss 0.581 Acc 0.971\n",
      "Epoch : 24 Loss 0.586 Acc 0.971\n",
      "Epoch : 25 Loss 0.578 Acc 0.981\n",
      "Epoch : 26 Loss 0.583 Acc 0.962\n",
      "Epoch : 27 Loss 0.578 Acc 0.981\n",
      "Epoch : 28 Loss 0.580 Acc 0.971\n",
      "Epoch : 29 Loss 0.580 Acc 0.971\n",
      "Epoch : 30 Loss 0.576 Acc 0.971\n",
      "Epoch : 31 Loss 0.580 Acc 0.981\n",
      "Epoch : 32 Loss 0.576 Acc 0.971\n",
      "Epoch : 33 Loss 0.578 Acc 0.971\n",
      "Epoch : 34 Loss 0.578 Acc 0.971\n",
      "Epoch : 35 Loss 0.575 Acc 0.971\n",
      "Epoch : 36 Loss 0.578 Acc 0.981\n",
      "Epoch : 37 Loss 0.575 Acc 0.981\n",
      "Epoch : 38 Loss 0.577 Acc 0.971\n",
      "Epoch : 39 Loss 0.575 Acc 0.971\n",
      "Epoch : 40 Loss 0.576 Acc 0.981\n",
      "Epoch : 41 Loss 0.575 Acc 0.981\n",
      "Epoch : 42 Loss 0.575 Acc 0.971\n",
      "Epoch : 43 Loss 0.575 Acc 0.971\n",
      "Epoch : 44 Loss 0.574 Acc 0.981\n",
      "Epoch : 45 Loss 0.574 Acc 0.981\n",
      "Epoch : 46 Loss 0.574 Acc 0.981\n",
      "Epoch : 47 Loss 0.574 Acc 0.981\n",
      "Epoch : 48 Loss 0.574 Acc 0.981\n",
      "Epoch : 49 Loss 0.573 Acc 0.981\n",
      "Epoch : 50 Loss 0.574 Acc 0.981\n",
      "Epoch : 51 Loss 0.573 Acc 0.981\n",
      "Epoch : 52 Loss 0.574 Acc 0.981\n",
      "Epoch : 53 Loss 0.573 Acc 0.981\n",
      "Epoch : 54 Loss 0.573 Acc 0.981\n",
      "Epoch : 55 Loss 0.573 Acc 0.981\n",
      "Epoch : 56 Loss 0.573 Acc 0.981\n",
      "Epoch : 57 Loss 0.573 Acc 0.981\n",
      "Epoch : 58 Loss 0.573 Acc 0.981\n",
      "Epoch : 59 Loss 0.572 Acc 0.981\n",
      "Epoch : 60 Loss 0.572 Acc 0.981\n",
      "Epoch : 61 Loss 0.572 Acc 0.981\n",
      "Epoch : 62 Loss 0.572 Acc 0.981\n",
      "Epoch : 63 Loss 0.572 Acc 0.990\n",
      "Epoch : 64 Loss 0.572 Acc 0.990\n",
      "Epoch : 65 Loss 0.572 Acc 0.981\n",
      "Epoch : 66 Loss 0.571 Acc 0.990\n",
      "Epoch : 67 Loss 0.571 Acc 0.990\n",
      "Epoch : 68 Loss 0.571 Acc 0.981\n",
      "Epoch : 69 Loss 0.571 Acc 0.990\n",
      "Epoch : 70 Loss 0.571 Acc 0.981\n",
      "Epoch : 71 Loss 0.571 Acc 0.990\n",
      "Epoch : 72 Loss 0.571 Acc 0.990\n",
      "Epoch : 73 Loss 0.571 Acc 0.990\n",
      "Epoch : 74 Loss 0.571 Acc 0.990\n",
      "Epoch : 75 Loss 0.571 Acc 0.990\n",
      "Epoch : 76 Loss 0.571 Acc 0.990\n",
      "Epoch : 77 Loss 0.570 Acc 0.990\n",
      "Epoch : 78 Loss 0.571 Acc 0.981\n",
      "Epoch : 79 Loss 0.571 Acc 0.990\n",
      "Epoch : 80 Loss 0.572 Acc 0.981\n",
      "Epoch : 81 Loss 0.571 Acc 0.990\n",
      "Epoch : 82 Loss 0.571 Acc 0.981\n",
      "Epoch : 83 Loss 0.571 Acc 0.990\n",
      "Epoch : 84 Loss 0.571 Acc 0.981\n",
      "Epoch : 85 Loss 0.571 Acc 0.990\n",
      "Epoch : 86 Loss 0.572 Acc 0.981\n",
      "Epoch : 87 Loss 0.570 Acc 0.990\n",
      "Epoch : 88 Loss 0.570 Acc 0.981\n",
      "Epoch : 89 Loss 0.570 Acc 0.990\n",
      "Epoch : 90 Loss 0.572 Acc 0.981\n",
      "Epoch : 91 Loss 0.571 Acc 0.990\n",
      "Epoch : 92 Loss 0.572 Acc 0.981\n",
      "Epoch : 93 Loss 0.570 Acc 0.990\n",
      "Epoch : 94 Loss 0.569 Acc 0.981\n",
      "Epoch : 95 Loss 0.569 Acc 0.990\n",
      "Epoch : 96 Loss 0.570 Acc 0.981\n",
      "Epoch : 97 Loss 0.571 Acc 0.981\n",
      "Epoch : 98 Loss 0.574 Acc 0.971\n",
      "Epoch : 99 Loss 0.569 Acc 0.981\n",
      "Epoch : 100 Loss 0.580 Acc 0.971\n",
      "Epoch : 101 Loss 0.572 Acc 0.981\n",
      "Epoch : 102 Loss 0.571 Acc 0.981\n",
      "Epoch : 103 Loss 0.580 Acc 0.971\n",
      "Epoch : 104 Loss 0.570 Acc 0.981\n",
      "Epoch : 105 Loss 0.571 Acc 0.981\n",
      "Epoch : 106 Loss 0.576 Acc 0.971\n",
      "Epoch : 107 Loss 0.573 Acc 0.981\n",
      "Epoch : 108 Loss 0.571 Acc 0.981\n",
      "Epoch : 109 Loss 0.580 Acc 0.971\n",
      "Epoch : 110 Loss 0.569 Acc 0.981\n",
      "Epoch : 111 Loss 0.573 Acc 0.981\n",
      "Epoch : 112 Loss 0.569 Acc 0.990\n",
      "Epoch : 113 Loss 0.569 Acc 0.990\n",
      "Epoch : 114 Loss 0.570 Acc 0.981\n",
      "Epoch : 115 Loss 0.569 Acc 0.990\n",
      "Epoch : 116 Loss 0.569 Acc 0.981\n",
      "Epoch : 117 Loss 0.568 Acc 0.990\n",
      "Epoch : 118 Loss 0.568 Acc 0.990\n",
      "Epoch : 119 Loss 0.568 Acc 0.981\n",
      "Epoch : 120 Loss 0.569 Acc 0.990\n",
      "Epoch : 121 Loss 0.570 Acc 0.981\n",
      "Epoch : 122 Loss 0.569 Acc 0.990\n",
      "Epoch : 123 Loss 0.569 Acc 0.981\n",
      "Epoch : 124 Loss 0.568 Acc 0.990\n",
      "Epoch : 125 Loss 0.568 Acc 0.981\n",
      "Epoch : 126 Loss 0.568 Acc 0.990\n",
      "Epoch : 127 Loss 0.569 Acc 0.981\n",
      "Epoch : 128 Loss 0.569 Acc 0.990\n",
      "Epoch : 129 Loss 0.570 Acc 0.981\n",
      "Epoch : 130 Loss 0.568 Acc 0.990\n",
      "Epoch : 131 Loss 0.568 Acc 0.990\n",
      "Epoch : 132 Loss 0.568 Acc 0.990\n",
      "Epoch : 133 Loss 0.568 Acc 0.990\n",
      "Epoch : 134 Loss 0.569 Acc 0.981\n",
      "Epoch : 135 Loss 0.570 Acc 0.981\n",
      "Epoch : 136 Loss 0.573 Acc 0.971\n",
      "Epoch : 137 Loss 0.568 Acc 0.981\n",
      "Epoch : 138 Loss 0.582 Acc 0.962\n",
      "Epoch : 139 Loss 0.568 Acc 0.990\n",
      "Epoch : 140 Loss 0.573 Acc 0.971\n",
      "Epoch : 141 Loss 0.568 Acc 0.990\n",
      "Epoch : 142 Loss 0.567 Acc 0.990\n",
      "Epoch : 143 Loss 0.568 Acc 0.981\n",
      "Epoch : 144 Loss 0.570 Acc 0.981\n",
      "Epoch : 145 Loss 0.574 Acc 0.971\n",
      "Epoch : 146 Loss 0.570 Acc 0.981\n",
      "Epoch : 147 Loss 0.585 Acc 0.962\n",
      "Epoch : 148 Loss 0.578 Acc 0.971\n",
      "Epoch : 149 Loss 0.589 Acc 0.971\n",
      "Epoch : 150 Loss 0.583 Acc 0.971\n",
      "Epoch : 151 Loss 0.568 Acc 0.990\n",
      "Epoch : 152 Loss 0.581 Acc 0.962\n",
      "Epoch : 153 Loss 0.571 Acc 0.981\n",
      "Epoch : 154 Loss 0.571 Acc 0.981\n",
      "Epoch : 155 Loss 0.574 Acc 0.971\n",
      "Epoch : 156 Loss 0.570 Acc 0.981\n",
      "Epoch : 157 Loss 0.568 Acc 0.981\n",
      "Epoch : 158 Loss 0.576 Acc 0.971\n",
      "Epoch : 159 Loss 0.571 Acc 0.981\n",
      "Epoch : 160 Loss 0.571 Acc 0.981\n",
      "Epoch : 161 Loss 0.574 Acc 0.971\n",
      "Epoch : 162 Loss 0.569 Acc 0.981\n",
      "Epoch : 163 Loss 0.568 Acc 0.981\n",
      "Epoch : 164 Loss 0.574 Acc 0.971\n",
      "Epoch : 165 Loss 0.571 Acc 0.981\n",
      "Epoch : 166 Loss 0.571 Acc 0.981\n",
      "Epoch : 167 Loss 0.573 Acc 0.981\n",
      "Epoch : 168 Loss 0.569 Acc 0.981\n",
      "Epoch : 169 Loss 0.568 Acc 0.981\n",
      "Epoch : 170 Loss 0.573 Acc 0.981\n",
      "Epoch : 171 Loss 0.571 Acc 0.981\n",
      "Epoch : 172 Loss 0.570 Acc 0.981\n",
      "Epoch : 173 Loss 0.572 Acc 0.981\n",
      "Epoch : 174 Loss 0.569 Acc 0.981\n",
      "Epoch : 175 Loss 0.568 Acc 0.981\n",
      "Epoch : 176 Loss 0.572 Acc 0.981\n",
      "Epoch : 177 Loss 0.571 Acc 0.981\n",
      "Epoch : 178 Loss 0.570 Acc 0.981\n",
      "Epoch : 179 Loss 0.571 Acc 0.981\n",
      "Epoch : 180 Loss 0.568 Acc 0.981\n",
      "Epoch : 181 Loss 0.568 Acc 0.981\n",
      "Epoch : 182 Loss 0.571 Acc 0.981\n",
      "Epoch : 183 Loss 0.570 Acc 0.981\n",
      "Epoch : 184 Loss 0.570 Acc 0.981\n",
      "Epoch : 185 Loss 0.572 Acc 0.981\n",
      "Epoch : 186 Loss 0.568 Acc 0.981\n",
      "Epoch : 187 Loss 0.568 Acc 0.981\n",
      "Epoch : 188 Loss 0.571 Acc 0.981\n",
      "Epoch : 189 Loss 0.570 Acc 0.981\n",
      "Epoch : 190 Loss 0.569 Acc 0.981\n",
      "Epoch : 191 Loss 0.573 Acc 0.981\n",
      "Epoch : 192 Loss 0.568 Acc 0.981\n",
      "Epoch : 193 Loss 0.568 Acc 0.981\n",
      "Epoch : 194 Loss 0.572 Acc 0.981\n",
      "Epoch : 195 Loss 0.569 Acc 0.981\n",
      "Epoch : 196 Loss 0.568 Acc 0.981\n",
      "Epoch : 197 Loss 0.573 Acc 0.981\n",
      "Epoch : 198 Loss 0.569 Acc 0.981\n",
      "Epoch : 199 Loss 0.568 Acc 0.981\n"
     ]
    }
   ],
   "source": [
    "for i in range(200):\n",
    "    \n",
    "    optim.zero_grad()\n",
    "    out = model.forward(input_train)\n",
    "    loss = criterion(out,output_train)\n",
    "    acc = torch.sum(torch.argmax(out,1) == output_train).float() / output_train.size()[0]\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    print(\"Epoch : %d Loss %.3f Acc %.3f\"%(i, loss.item(), acc.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadron/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "pred = model.forward(input_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = torch.sum(torch.argmax(pred,1) == output_test).float() / output_test.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9778)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 2]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [2 2]\n",
      " [2 2]\n",
      " [2 2]\n",
      " [0 0]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [2 2]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [2 2]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [2 2]\n",
      " [0 0]\n",
      " [1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(np.column_stack((torch.argmax(pred,1), output_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
